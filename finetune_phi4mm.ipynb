{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Deep-unlearning/notebooks/blob/main/finetune_phi4mm.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Phi-4 for ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will fine-tune pretrained Phi-4-Multimodal-Instruct on a small split of earnings22 test set.\n",
    "\n",
    "Let's get started by installing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub==0.25.2\n",
    "!pip install scipy==1.15.1\n",
    "!pip install peft==0.13.2\n",
    "!pip install backoff==2.2.1\n",
    "!pip install transformers==4.46.1\n",
    "!pip install accelerate==1.3.0\n",
    "!pip install sacrebleu\n",
    "!pip install torchvision\n",
    "!pip install hf_transfer\n",
    "!pip install transformers\n",
    "!pip install librosa\n",
    "!pip install soundfile\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import jiwer\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    BatchFeature,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "\n",
    "# Fixed ASR instruction and other constants\n",
    "INSTRUCTION = \"Transcribe the audio.\"\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESBEarnings22Dataset(Dataset):\n",
    "    def __init__(self, processor, dataset, training=True):\n",
    "        \"\"\"\n",
    "        processor: the AutoProcessor instance\n",
    "        dataset: a Hugging Face Dataset (already split into train/validation)\n",
    "        training: whether this dataset is for training (affects concatenation of target tokens)\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.training = training\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTRUCTION\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        # The dataset contains an \"audio\" dict and a \"text\" field for transcription.\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(\n",
    "            text=prompt,\n",
    "            audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])],\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        answer = f\"{data['text']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if self.training:\n",
    "            # Concatenate prompt and answer, but mask all tokens except the answer.\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1]:] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(t.dim() == ndim for t in tensors[1:]), 'All tensors must have the same number of dimensions'\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esb_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1 else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "    return BatchFeature({\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels,\n",
    "        'attention_mask': attention_mask,\n",
    "        'input_audio_embeds': input_audio_embeds,\n",
    "        'audio_embed_sizes': audio_embed_sizes,\n",
    "        'audio_attention_mask': audio_attention_mask,\n",
    "        'input_mode': 2,  # speech mode\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "        return torch.all(self.stop_tokens_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=esb_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'):\n",
    "        stopping_criteria = StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx],\n",
    "                               skip_special_tokens=True,\n",
    "                               clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).rstrip(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        wer = jiwer.wer(all_labels, all_generated_texts)\n",
    "        print(\"WER:\", wer)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'wer': wer,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "        return wer\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "MODEL_NAME = 'microsoft/Phi-4-multimodal-instruct'\n",
    "OUTPUT_DIR = './output/'\n",
    "USE_FLASH_ATTENTION = False\n",
    "BATCH_SIZE_PER_GPU = 2\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LEARNING_RATE = 4.0e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Initialize Accelerator for potential multi-GPU training.\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Load processor and model\n",
    "with accelerator.local_main_process_first():\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    model = create_model(MODEL_NAME, use_flash_attention=USE_FLASH_ATTENTION)\n",
    "\n",
    "# Set LoRA adapter (if available) for speech tasks.\n",
    "model.set_lora_adapter('speech')\n",
    "\n",
    "# Load and split the dataset.\n",
    "ds_full = load_dataset(\"hf-audio/esb-datasets-test-only-sorted\", \"earnings22\", split=\"test\")\n",
    "split_ds = ds_full.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = split_ds[\"train\"]\n",
    "val_ds = split_ds[\"test\"]\n",
    "\n",
    "# Create dataset objects.\n",
    "train_dataset = ESBEarnings22Dataset(processor, train_ds, training=True)\n",
    "val_dataset = ESBEarnings22Dataset(processor, val_ds, training=False)\n",
    "\n",
    "num_gpus = accelerator.num_processes\n",
    "print(f\"Training on {num_gpus} GPUs\")\n",
    "\n",
    "# Compute gradient accumulation steps (for multi-GPU training).\n",
    "gradient_accumulation_steps = (BATCH_SIZE_PER_GPU * num_gpus) // BATCH_SIZE_PER_GPU\n",
    "\n",
    "# Set mixed precision flags.\n",
    "fp16 = not USE_FLASH_ATTENTION\n",
    "bf16 = USE_FLASH_ATTENTION\n",
    "\n",
    "# Define training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_GPU,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim='adamw_torch',\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.95,\n",
    "    adam_epsilon=1e-7,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy='no',\n",
    "    save_total_limit=10,\n",
    "    save_only_model=True,\n",
    "    bf16=bf16,\n",
    "    fp16=fp16,\n",
    "    remove_unused_columns=False,\n",
    "    report_to='none',\n",
    "    dataloader_num_workers=4,\n",
    "    ddp_find_unused_parameters=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist.\n",
    "Path(training_args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Evaluate the model before fine-tuning.\n",
    "print(\"Evaluating before fine-tuning...\")\n",
    "wer_before = evaluate(\n",
    "    model,\n",
    "    processor,\n",
    "    val_dataset,\n",
    "    save_path=Path(training_args.output_dir) / 'eval_before.json',\n",
    "    eval_batch_size=BATCH_SIZE_PER_GPU,\n",
    ")\n",
    "print(f\"WER before fine-tuning: {wer_before}\")\n",
    "\n",
    "# Setup the Trainer for fine-tuning.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=esb_collate_fn,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory before re-loading the model.\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload the fine-tuned model.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    training_args.output_dir,\n",
    "    torch_dtype=torch.bfloat16 if USE_FLASH_ATTENTION else torch.float32,\n",
    "    trust_remote_code=True,\n",
    "    _attn_implementation='flash_attention_2' if USE_FLASH_ATTENTION else 'sdpa',\n",
    ").to('cuda')\n",
    "\n",
    "# Evaluate the model after fine-tuning.\n",
    "print(\"Evaluating after fine-tuning...\")\n",
    "wer_after = evaluate(\n",
    "    model,\n",
    "    processor,\n",
    "    val_dataset,\n",
    "    save_path=Path(training_args.output_dir) / 'eval_after.json',\n",
    "    eval_batch_size=BATCH_SIZE_PER_GPU,\n",
    ")\n",
    "print(f\"WER after fine-tuning: {wer_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WER before finetuning: 0.20291545189504373\n",
    "\n",
    "\n",
    "WER after finetuning: 0.1607385811467444"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (router)",
   "language": "python",
   "name": "router"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
